\documentclass{article}

% Language setting
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{titling} % For subtitle support

\title{Linear Models\\
\large Advanced Machine Learning}
\author{Yadder Aceituno}

\begin{document}
\maketitle


\section{Content of Course}

\begin{enumerate}
    \item Linear or Logistic
    \item Decision Trees
    \item SVM
    \item Neural Networks
    \item Representational Issues
    \item Deep Learning
    \item Gaussian Processes
\end{enumerate}

\section{Lecture 1: Linear Regression}

\subsection{Linear Regression}

Given a set of data points $<x_i, y_i>$, the goal is to find the weights 
$\theta$ such that $X_i \theta = Y_i$.

Where:

$$X = \langle 1, x_1, x_2, \ldots, x_m \rangle$$
$$Y =  \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_m x_m$$

\subsection{Maximum Likelihood Estimation}

Assuming that data points are independent and identically distributed, 
the goal is to find the parameters $\theta$ that maximize the likelihood 
of the data with the next equation:
$$
\theta = argmax_{\theta_0, \theta_1, \ldots, \theta_m} \prod_{i=1}^{N} p(y_i | x_i; \theta)
$$


Where 
$$
p(y_i | x_i; \theta) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}}
$$
$$
\mu_i = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_m x_m
$$

If we assume that the data points are independent and identically distributed, 
then the probability of observing all data points is the product of their 
individual probabilities. This is the joint likelihood. This method can derive into the Mean Squared Error (MSE). This can be proven
as follows:

Because it's equivalent to maximize the likelihood, we can maximize the log-likelihood:

$$
\theta = argmax_{\theta_0, \theta_1, \ldots, \theta_m} \log \left( \prod_{i=1}^{N} p(y_i | x_i; \theta) \right)
$$

Because the logaritm of a product is the sum of the logaritms, we can rewrite the equation as:
$$
\theta = argmax_{\theta_0, \theta_1, \ldots, \theta_m} \sum_{i=1}^{N} \log p(y_i | x_i; \theta)
$$

Further we can transform the product in the logaritm into a sum of logaritms:
$$
\theta = argmax_{\theta_0, \theta_1, \ldots, \theta_m} \sum_{i=1}^{N} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \right) + \log \left( e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}} \right)
$$

Here, the first term in the sum is a constant that doesn't depend on $\theta$, so it can be ignored.
In the second term, the logaritm of the exponential is the exponential itself, so we can rewrite the equation as:
$$
\theta = argmax_{\theta_0, \theta_1, \ldots, \theta_m} \sum_{i=1}^{N} -\frac{(y_i - \mu_i)^2}{2\sigma^2}
$$

Which is the same as minimizing the Mean Squared Error (MSE).

\subsection{Normal Equation}

Method to find the optimal parameters.
The equation for finding the optimal parameters is: 
$$
\theta = (X^T X)^{-1} X^T y
$$

How we got this equation?

First we need to define the cost function that we want to minimize, which is the Mean Squared Error (MSE):
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2
$$

Writing it in matrix form:
$$
J(\theta) = \frac{1}{2m} (X \theta - y)^T (X \theta - y)
$$

So, we need to minimize the cost function $J(\theta)$. We can do this by taking the derivative of $J(\theta)$ 
with respect to $\theta$ and setting it to zero:

$$
\frac{\partial J(\theta)}{\partial \theta} =  \frac{\partial}{\partial \theta} (X\theta - y)^T (X \theta - y) = X^T (X \theta - y)
$$

So, our gradient is:
$$
\nabla J(\theta) = X^T (X \theta - y)
$$

If we set the gradient to zero, we get:

$$
X^T (X \theta - y) = 0
$$

Solving for $\theta$:
$$
X^T X \theta = X^T y
$$
$$
\theta = (X^T X)^{-1} X^T y
$$

And that's the \textbf{normal equation}.

\subsection{Gradient Descent}

Iterative method to find the min/max optimal parameters of a function.

The steps are:
\begin{enumerate}
    \item Start with random parameters
    \item Iteratively update them to minimize the cost function.
    \item Keep changing the parameters until the cost function is minimized.
\end{enumerate}


The equation to repeat until convergence is: 
$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_{ij}
$$

Where $\alpha$ is the learning rate.
And $x_{ij}$ is the $j$-th feature of the $i$-th training example.
There's an strategy where the learning rate can start with a high 
value and then decrease over time.

Some strategies:
\begin{itemize}
    \item Batch Gradient Descent
    Updates the parameters using the entire dataset. Computationally expensive.
    \item Stochastic Gradient Descent (SGD)
    Updates the parameters using one training example at a time. Computationally efficient but less smooth convergence.
    \item Mini-batch Gradient Descent
    Updates the parameters using a small subset of the dataset. Computationally efficient and smooth convergence.
\end{itemize}

\textbf{Learning rate}

If the learning rate is too small, the algorithm will take a long time to converge.
If the learning rate is too large, the algorithm may not converge.

\textbf{Overfitting}

High number of features means high number of heights
\begin{itemize}
    \item Reduce the number of features
    \item Regularization
\end{itemize}

\section{Logistic Regression}

Cast a binary classification problem as a regression problem. It uses the sigmoid function to map the output to a probability.

Sigmoid function: $\sigma(z) = \frac{1}{1 + e^{-z}}$

Cost function: 
$$
Cost(h_\theta(x), y) = -y \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))
$$


\section{Regression as a shallow network}
Same math as modern

Summary: 
- Linear Regression: $h_\theta(x) = \theta_0 + \theta_1 x$
- Logistic Regression: $h_\theta(x) = \sigma(\theta_0 + \theta_1 x)$
- Softmax regression vs k-binary classifiers



Lecture 2

Bias per layer or per input (neuron)

Random for weights is important because if no the neurons will behave the same.

* element wise product (operation between two matrices of the same size)

Hopfield Network
- Binary Units
- Symmetric Connections
- Energy Function
\end{document}