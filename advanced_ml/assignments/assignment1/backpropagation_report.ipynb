{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38925b8",
   "metadata": {},
   "source": [
    "# Neural Networks: Implementing Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea22302",
   "metadata": {},
   "source": [
    "## Code explanation\n",
    "\n",
    "### Description\n",
    "This project is a simple implementation of a neural network built from scratch using NumPy. It is designed to demonstrate the core concepts of forward propagation, backpropagation, and gradient descent.\n",
    "\n",
    "### Structure\n",
    "**1. Helper Functions:**\n",
    "- get_activation(name): Returns a tuple containing the specified activation function (func) and its gradient (grad).\n",
    "- get_loss(name): Returns a tuple containing the loss function (loss) and its gradient (grad).\n",
    "\n",
    "**2. MyNeuralNetwork Class**\n",
    "- __init__(): Initializes the network. Sets up layer sizes, randomly initializes weights, sets biases to zero, and prepares the activation/loss functions. It also calls initialize_log_file().\n",
    "- initialize_log_file(): Creates the CSV log file and writes the header row. The header includes epoch, loss, accuracy, and dynamic columns for the Z-value (pre-activation) and activation-value of each neuron in the hidden layer (e.g., z_hidden_0, a_hidden_0, ...).\n",
    "- log_training_step(): Appends a new row of data to the CSV file for the current training step.\n",
    "- forward(X): Performs a forward pass through the network, calculating and storing the hidden layer and output layer values. Returns the final predictions and the hidden layer's Z-values.\n",
    "- backward(X, y, y_hat, ...): Performs backpropagation to calculate the gradients of the loss with respect to all weights and biases using the chain rule.\n",
    "- train(X, y, ...): The main training loop. It iterates for the specified number of epochs, performs forward and backward passes, updates the weights and biases, and logs progress to the console (every 100 epochs) and the CSV file (every epoch).\n",
    "\n",
    "**3. Training Log**\n",
    "- The output CSV file (default: statistics.csv) provides a complete, epoch-by-epoch trace of the network's internal state.\n",
    "\n",
    "### How to use it\n",
    "1. Prepare your training data: The network needs input data (X) and target labels (y).\n",
    "2. Initialize the Neural Network: Create an instance of the MyNeuralNetwork class. Define your network's architecture and settings.\n",
    "3. Train the network: Call the .train() method to begin training. Pass your data (X_train, y_train) and specify the epochs (how many times to loop over the data) and the learning_rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc63eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Activation and loss functions\n",
    "def get_activation(name):\n",
    "    if name == \"sigmoid\":\n",
    "        def func(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        def grad(x):\n",
    "            return x * (1 - x)\n",
    "    elif name == \"relu\":\n",
    "        def func(x):\n",
    "            return np.maximum(0, x)\n",
    "        def grad(x):\n",
    "            return (x > 0).astype(float)\n",
    "    elif name == \"tanh\":\n",
    "        def func(x):\n",
    "            return np.tanh(x)\n",
    "        def grad(x):\n",
    "            return 1 - np.square(x)\n",
    "    elif name == \"softmax\":\n",
    "        def func(x):\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        def grad(x):\n",
    "            return x * (1 - x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation: {name}\")\n",
    "    return func, grad\n",
    "\n",
    "def get_loss(name):\n",
    "    if name == \"mean_squared_error\":\n",
    "        def loss(y_true, y_pred):\n",
    "            return np.mean((y_true - y_pred) ** 2)\n",
    "        def grad(y_true, y_pred):\n",
    "            return 2 * (y_pred - y_true) / y_true.size\n",
    "    elif name == \"cross_entropy\":\n",
    "        def loss(y_true, y_pred):\n",
    "            eps = 1e-7\n",
    "            y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "            return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        def grad(y_true, y_pred):\n",
    "            # gradient for softmax + cross-entropy simplification\n",
    "            return (y_pred - y_true) / y_true.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {name}\")\n",
    "    return loss, grad\n",
    "\n",
    "# Define the Neural Network class\n",
    "class MyNeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size = 8, \n",
    "        hidden_size = 3, \n",
    "        output_size = 8, \n",
    "        hidden_activation=\"sigmoid\",\n",
    "        output_activation=\"softmax\",\n",
    "        loss_function=\"cross_entropy\",\n",
    "        output_file = 'statistics.csv'\n",
    "    ):\n",
    "        \n",
    "        # Generating the weights\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "        # Generating the biases\n",
    "        self.bias_input_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_hidden_output = np.zeros((1, output_size))\n",
    "        \n",
    "        # Get activation and loss functions\n",
    "        self.hidden_act, self.hidden_grad = get_activation(hidden_activation)\n",
    "        self.output_act, self.output_grad = get_activation(output_activation)\n",
    "        self.loss_func, self.loss_grad = get_loss(loss_function)\n",
    "\n",
    "        # Defining the output file for the statistics\n",
    "        self.output_file = output_file\n",
    "\n",
    "        # Initialize log file\n",
    "        self.initialize_log_file()\n",
    "\n",
    "    def initialize_log_file(self):\n",
    "        # Create headers for the CSV file\n",
    "        headers = ['epoch', 'loss', 'accuracy']\n",
    "        \n",
    "        # Add columns for each hidden neuron's Z and activation\n",
    "        for i in range(self.weights_input_hidden.shape[1]):  # For each hidden neuron\n",
    "            headers.extend([f'z_hidden_{i}', f'a_hidden_{i}'])\n",
    "        \n",
    "        # Write headers to file\n",
    "        with open(self.output_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(headers)\n",
    "\n",
    "    def log_training_step(self, epoch, loss, accuracy, hidden_z, hidden_activations):\n",
    "        # Prepare row data\n",
    "        row = [epoch, loss, accuracy]\n",
    "        # Add Z and activation for each hidden neuron\n",
    "        for z, a in zip(hidden_z[0], hidden_activations[0]):\n",
    "            row.extend([z, a])\n",
    "        \n",
    "        # Append to CSV\n",
    "        with open(self.output_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Calculating the hidden layer\n",
    "        # Store Z values\n",
    "        self.z_hidden = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n",
    "        # Calculate activation\n",
    "        self.hidden_layer = self.hidden_act(self.z_hidden)\n",
    "\n",
    "        # Calculating the output layer\n",
    "        self.z_output = np.dot(self.hidden_layer, self.weights_hidden_output) + self.bias_hidden_output\n",
    "        self.output_layer = self.output_act(self.z_output)\n",
    "        \n",
    "        return self.output_layer, self.z_hidden\n",
    "\n",
    "    def backward(self, X, y, y_hat, learning_rate = 0.01):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # ================================= [Output layer error] =================================\n",
    "        # For loss function we use MSE\n",
    "        # The loss function derivative with respect to the predicted values y_hat\n",
    "        dL_dyhat = self.loss_grad(y, y_hat)\n",
    "        # The derivative of the predicted values with respect to z (W * X + b)\n",
    "        # is the derivative of sigmoid\n",
    "        d_act_output = self.output_grad(y_hat)\n",
    "        # The derivative of the loss function with respect to z \n",
    "        # is the product of the loss function derivative and the sigmoid derivative\n",
    "        # This is the error term\n",
    "        error_term_output = dL_dyhat * d_act_output\n",
    "\n",
    "        # The derivative of the loss function with respect to weights\n",
    "        # is the product of the error term and the hidden layer\n",
    "        # and then we divide by the number of samples\n",
    "        dW_hidden_output = np.dot(self.hidden_layer.T, error_term_output) / m\n",
    "        # The derivative of the loss function with respect to bias\n",
    "        # is the sum of the error term\n",
    "        # and then we divide by the number of samples\n",
    "        db_hidden_output = np.sum(error_term_output, axis=0, keepdims=True) / m\n",
    "\n",
    "        # ================================= [Hidden layer error] =================================\n",
    "        # The derivative of the loss function with respect to weights\n",
    "        \n",
    "        error_term_hidden = np.dot(error_term_output, self.weights_hidden_output.T) * self.hidden_grad(self.hidden_layer)\n",
    "\n",
    "        # The derivative of the loss function with respect to weights\n",
    "        dW_input_hidden = np.dot(X.T, error_term_hidden) / m\n",
    "        # The derivative of the loss function with respect to bias\n",
    "        db_input_hidden = np.sum(error_term_hidden, axis=0, keepdims=True) / m\n",
    "\n",
    "        # ================================= [Update weights and biases] ==========================\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output -= learning_rate * dW_hidden_output\n",
    "        self.bias_hidden_output -= learning_rate * db_hidden_output\n",
    "\n",
    "        self.weights_input_hidden -= learning_rate * dW_input_hidden\n",
    "        self.bias_input_hidden -= learning_rate * db_input_hidden\n",
    "        \n",
    "    def train(self, X, y, epochs=10000, learning_rate=0.01):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_hat, z_hidden = self.forward(X)\n",
    "\n",
    "            self.backward(X, y, y_hat, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                # Calculate metrics\n",
    "                loss = self.loss_func(y, y_hat)\n",
    "                predictions = np.argmax(y_hat, axis=1)\n",
    "                accuracy = np.mean(predictions == np.argmax(y, axis=1))\n",
    "                \n",
    "                # Log to console\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Log to CSV\n",
    "            self.log_training_step(\n",
    "                epoch=epoch,\n",
    "                loss=loss,\n",
    "                accuracy=accuracy,\n",
    "                hidden_z=z_hidden,\n",
    "                hidden_activations=self.hidden_layer\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d7b80",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "input_size = 8\n",
    "hidden_size = 3\n",
    "output_size = 8\n",
    "\n",
    "X = np.array([\n",
    "[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 1]\n",
    "])\n",
    "y = np.array([\n",
    "[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the neural network - Sigmoid activation and MSE loss\n",
    "nn = MyNeuralNetwork(input_size=input_size, \n",
    "                     hidden_size=hidden_size, \n",
    "                     output_size=output_size,\n",
    "                     hidden_activation=\"sigmoid\",\n",
    "                     output_activation=\"sigmoid\",\n",
    "                     loss_function=\"mean_squared_error\",\n",
    "                     output_file='statistics_sigmoid_mse.csv'\n",
    "                     )\n",
    "nn.train(X, y, epochs=10000, learning_rate=0.2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
