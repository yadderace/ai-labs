\subsection*{Overview}
The detection and correction of data quality problems. The use of algorithms that can tolerate poor data quality. Data can be:
\begin{itemize}[noitemsep]
    \item Inconsistent: data transformations, technology problems, human errors
    \item Incomplete: missing values, incomplete records
    \item Inaccurate: errors in data entry, data transformations, technology problems
    \item Outdated: data transformations, technology problems
\end{itemize}

\subsection*{Data Cleaning}
Converting data so that it becomes consistent, complete, accurate, and up-to-date. It's realized by filling missing values, removing duplicates, smoothing noise, and resolving inconsistencies.

\subsubsection*{Handling Noisy Data}
\begin{itemize}[noitemsep]
    \item Clustering (detect and remove outliers)
    \item Computer and human inspection
\end{itemize}

\subsubsection*{Handling Missing Data}
\begin{itemize}[noitemsep]
    \item Filling manually
    \item Using the variable mode, median, or mean
\end{itemize}

\subsection*{Data Integration}
Combining data from different sources.

\subsubsection*{Possible Problems}
\begin{itemize}[noitemsep]
    \item Different variables have the same name
    \item Similar variables have different names
    \item Redundant variable: can be detected with Chi-Square, Covariance analysis
\end{itemize}

\subsubsection*{Chi-square Test}
$$\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$$

\subsubsection*{Covariance}
$$\text{Cov}(X, Y) = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu_x)(y_i - \mu_y)$$

\subsection*{Data Reduction}
Obtaining a reduced set of variables that are sufficient to represent the data.

\subsubsection*{Strategies}
\begin{itemize}[noitemsep]
    \item \textbf{Principal Component Analysis (PCA)}: Converts variables into a new set of variables that are uncorrelated and capture the maximum variance.
    \item \textbf{Multidimensional Scaling (MDS)}: Finds a low-dimensional representation that preserves pairwise distances (or dissimilarities) between points.
    \item \textbf{Feature Selection}: Select a subset of variables that are most relevant to the task.
    \item \textbf{Clustering}: Group similar objects together.
    \item \textbf{Sampling}: Main strategy for data reduction in data mining. The sample must be representative of the population.
    \begin{itemize}
        \item Without replacement: each object is selected only once
        \item With replacement: each object can be selected multiple times
        \item Stratified sampling: data is split into partitions and a sample is taken from each partition
    \end{itemize}
\end{itemize}

\subsection*{Data Valuation}
Seeks to assign a numerical value to an individual's data in the trade of data. The issue is the time and cost of data valuation. Complexity is above $O(2^N)$.

\subsection*{Data Transformation and Discretization}
A function that maps the entire set of values of a given variable to a new set of replacement values.

\subsubsection*{Methods}
\begin{itemize}[noitemsep]
    \item \textbf{Normalization}: Scales the values to a range, such as [0, 1] or [-1, 1].
    \item \textbf{Smoothing}: Reduces noise in the data.
    \item \textbf{Variable/Feature Construction}: Creates new variables from existing variables.
\end{itemize}

\subsubsection*{Normalization}
\begin{itemize}[noitemsep]
    \item \textbf{Min-Max}: Scales the values to a range, such as [0, 1] or [New Min, New Max]
    $$\bar{x} = \frac{x - \min(x)}{\max(x) - \min(x)}$$
    
    \item \textbf{Z-Score}: Scales the values to have a mean of 0 and a standard deviation of 1.
    $$\bar{x} = \frac{x - \mu}{\sigma}$$
\end{itemize}

\subsubsection*{Discretization}
Divides the range of continuous values into a set of intervals. The intervals are called bins and can replace the original values. Clustering can also be used to find the intervals.
\begin{itemize}[noitemsep]
    \item \textbf{Binning}: Can be done with equal width or equal frequency (depth)
\end{itemize}
